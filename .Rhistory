"HerbertHoover","JohnFKennedy","RichardNixon","WoodrowWilson",
"AbrahamLincoln", "TheodoreRoosevelt", "JamesGarfield",
"JohnQuincyAdams", "UlyssesSGrant", "ThomasJefferson",
"GeorgeWashington", "WilliamHowardTaft", "AndrewJackson",
"WilliamHenryHarrison", "JohnAdams")
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
j
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
View(tidy_books)
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],linenumber=1:length(docs[[i]][[3]]))
}
docs[[1]][[3]]
docs[[1]][[3]][,1]
docs[[1]][[3]][,2]
?tibble
View(farewell.list)
View(farewell.list)
View(farewell)
View(farewell.list)
View(letters)
View(tidy_books)
View(docs[[1]][[3]])
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],linenumber=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-docs[[i]][[3]] %>% unnest_tokens(word,text)
}
View(tidy_books)
?unnest_tokens
d <- data_frame(txt = prideprejudice)
View(d)
d %>%
unnest_tokens(word, txt)
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],linenumber=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],text)
}
View(docs[[1]][[3]])
d %>%
unnest_tokens(sentence, txt, token = "sentences")
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],linenumber=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],linenumber=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],text,word)
}
unnest_tokens(docs[[1]][[3]],word,text)
unnest_tokens(docs[[1]][[3]],word,text)
docs[[1]][[3]]$text
mode(docs[[1]][[3]]$text)
docs[[1]][[3]]$text[1]
?sent_detect
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
##docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
View(docs[[1]][[3]])
View(docs[[1]])
docs[[1]][[3]]
View(docs[[1]][[3]])
mode(docs[[1]][[3]])
mode(docs[[1]][[3]][1])
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
docs[[1]][[5]]
docs[[1]][[4]]
docs[[1]][[5]]<-1
unnest_tokens(tidy_books,word,text)
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],,line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]],as.list=TRUE)
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=as.character(docs[[i]][[3]]),line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
unnest_tokens(tidy_books,word,text)
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=as.character(docs[[i]][[3]]),line=1:length(docs[[i]][[3]]))
#docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
summary(docs[[1]])[[3]]
summary(docs[[1]][[3]])
summary(docs[[1]][[3]][1])
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=as.character(docs[[i]][[3]]),line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]],as.list=T)
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]],as.list=TRUE)
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],line=1:length(docs[[i]][[3]]))
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
names(docs[[i]][[3]])<-text
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
}
for(i in 1:58){
docs[[i]][[3]]<-unlist(sent_detect(docs[[i]][[1]],as.list=TRUE))
names(docs[[i]][[3]])<-text
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],input = text)
}
View(docs)
View(dtms)
View(dtms.list)
dtms.list[[1]]
dtms.list[[1]]$dtm
dtms.list[[1]]$freq
dtms.list[[1]]$freq<-data.frame(word=names(dtms.list[[1]]$freq),fq=dtms.list[[1]]$freq)
dtms.list[[1]]$freq
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
dtms.list[[i]]$freq<-data.frame(word=names(dtms.list[[i]]$freq),fq=dtms.list[[i]]$freq)
rownames(dtms.list[[i]]$freq)<-NULL
}
dtms.list[[1]]$freq
dtms.list[[2]]$freq
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(plyr)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(lda)
library(reshape2)
library(tidytext)
library(stringr)
library(qdap)
library(beeswarm)
path<-file.path(".","data","InauguralSpeeches")
filenames<-dir(path)
speeches.number<-length(dir(path))
docs<-Corpus(DirSource(path))
head(summary(docs),3)
docs<-tm_map(docs,content_transformer(tolower))
punc.count<-function(speeches){
text<-speeches[[1]]
fullstop<-length(gregexpr("[.]",text)[[1]])
comma<-length(gregexpr(",",text)[[1]])
exclamation<-length(gregexpr("!",text)[[1]])
question<-length(gregexpr("[?]",text)[[1]])
semicolon<-length(gregexpr(";",text)[[1]])
numbers<-length(gregexpr("[0-9]+",text)[[1]])
i<-length(gregexpr("i |me |myself |my |mine ",text)[[1]])
we<-length(gregexpr("we |us |ourself|our |ours ",text)[[1]])
you<-length(gregexpr("you |your |yourself|yours ",text)[[1]])
n<-fullstop+exclamation+question+semicolon
return(list(sentence=n,fullstop=fullstop,comma=comma,exclamation=exclamation,question=question,semicolon=semicolon,numbers=numbers,i=i,we=we,you=you))
}
punc.num<-laply(docs,punc.count)
docs1<-tm_map(docs,removeNumbers)
docs1<-tm_map(docs1,removePunctuation)
docs1<-tm_map(docs1,removeWords,stopwords("english"))
docs1<-tm_map(docs1,stripWhitespace)
dtmatrix<-DocumentTermMatrix(docs1)
freq<-colSums(as.matrix(dtmatrix))
head(table(freq))
dtmatrix.new<-removeSparseTerms(dtmatrix,0.8)
write.csv(as.matrix(dtmatrix),file="output/myoutput/dtmatrix.csv")
freq<-colSums(as.matrix(dtmatrix.new))
freqsorted<-sort(freq)
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
library(Rgraphviz)
pdf(file="output/myoutput/correlation plot.pdf")
plot(dtmatrix,terms=findFreqTerms(dtmatrix,lowfreq = 100)[1:30],corThreshold = 0.5)
findAssocs(dtmatrix,tail(names(freq),10),corlimit = 0.7)
dtms<-function(text){
corp<-Corpus(VectorSource(text))
dtm<-DocumentTermMatrix(corp)
fq<-colSums(as.matrix(dtm))
return(list(dtm=dtm,freq=fq))
}
dtms.list<-llply(docs1,dtms)
name<-rownames(summary(dtms.list))
name<-paste0("output/myoutput/wordcloud/",name)
name<-sub(".txt",".jpg",name)
for (i in 1:58){
jpeg(file=name[i])
wordcloud(names(dtms.list[[i]]$freq),dtms.list[[i]]$freq,min.freq = 50,max.words = 100,colors=brewer.pal(6,"Dark2"))
dev.off()
}
name<-sub("wordcloud","wordlength",name)
word<-list()
for(i in 1:58){
word[[i]]<-dtms.list[[i]][[1]] %>%
as.matrix %>%
colnames %>%
(function(x) x[nchar(x)<20])
letters<-data.frame(nletters=nchar(word[[i]]))
jpeg(file=name[i])
print(ggplot(letters,aes(nletters))+
geom_histogram(binwidth = 1)+
geom_vline(xintercept = mean(nchar(word[[i]])),
colour="green",size=1,alpha=0.5)+
labs(x="number of letters",y="number of words"))
dev.off()
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
dtms.list[[i]]$freq<-data.frame(word=names(dtms.list[[i]]$freq),fq=dtms.list[[i]]$freq)
rownames(dtms.list[[i]]$freq)<-NULL
}
?wordcloud
?data_frame
?data.frame
default.stringsAsFactors()
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]])
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
# dtms.list[[i]]$freq<-data.frame(word=names(dtms.list[[i]]$freq),fq=dtms.list[[i]]$freq,stringsAsFactors = FALSE)
# rownames(dtms.list[[i]]$freq)<-NULL
}
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],stringsAsFactors = FALSE)
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
# dtms.list[[i]]$freq<-data.frame(word=names(dtms.list[[i]]$freq),fq=dtms.list[[i]]$freq,stringsAsFactors = FALSE)
# rownames(dtms.list[[i]]$freq)<-NULL
}
docs[[i]][[5]]
par(mar=c(4, 11, 2, 2))
#sel.comparison=levels(sentence.list$FileOrdered)
sentence.list.sel=filter(sentence.list,
type=="nomin", Term==1, File%in%sel.comparison)
sentence.list.sel$File=factor(sentence.list.sel$File)
sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File,
sentence.list.sel$word.count,
mean,
order=T)
beeswarm(word.count~FileOrdered,
data=sentence.list.sel,
horizontal = TRUE,
pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
cex=0.55, cex.axis=0.8, cex.lab=0.8,
spacing=5/nlevels(sentence.list.sel$FileOrdered),
las=2, xlab="Number of words in a sentence.", ylab="",
main="Nomination speeches")
par(mar=c(4, 11, 2, 2))
#sel.comparison=levels(sentence.list$FileOrdered)
sentence.list.sel=filter(sentence.list,
type=="nomin", Term==2, File%in%sel.comparison)
sentence.list.sel$File=factor(sentence.list.sel$File)
sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File,
sentence.list.sel$word.count,
mean,
order=T)
beeswarm(word.count~FileOrdered,
data=sentence.list.sel,
horizontal = TRUE,
pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
cex=0.55, cex.axis=0.8, cex.lab=0.8,
spacing=1.2/nlevels(sentence.list.sel$FileOrdered),
las=2, xlab="Number of words in a sentence.", ylab="",
main="Nomination speeches, 2nd term")
sentence.list%>%
filter(File=="DonaldJTrump",
type=="nomin",
word.count<=3)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="AlbertGore,Jr",
type=="nomin",
word.count<=3)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="Clinton",
type=="nomin",
word.count<=3)%>%
select(sentences)
sentence.list%>%
filter(File=="WilliamJClinton",
type=="nomin", Term==1,
word.count<=3)%>%
select(sentences)
sentence.list.sel=sentence.list%>%filter(type=="inaug", File%in%sel.comparison, Term==1)
sentence.list.sel$File=factor(sentence.list.sel$File)
sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File,
sentence.list.sel$word.count,
mean,
order=T)
par(mar=c(4, 11, 2, 2))
beeswarm(word.count~FileOrdered,
data=sentence.list.sel,
horizontal = TRUE,
pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6),
cex=0.55, cex.axis=0.8, cex.lab=0.8,
spacing=5/nlevels(sentence.list.sel$FileOrdered),
las=2, ylab="", xlab="Number of words in a sentence.",
main="Inaugural Speeches")
sentence.list%>%
filter(File=="BarackObama",
type=="inaug",
word.count<=3)%>%
select(sentences)
par(mfrow=c(4,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)
f.plotsent.len(In.list=sentence.list, InFile="HillaryClinton",
InType="nomin", InTerm=1, President="Hillary Clinton")
f.plotsent.len(In.list=sentence.list, InFile="DonaldJTrump",
InType="nomin", InTerm=1, President="Donald Trump")
f.plotsent.len(In.list=sentence.list, InFile="BarackObama",
InType="nomin", InTerm=1, President="Barack Obama")
f.plotsent.len(In.list=sentence.list, InFile="GeorgeWBush",
InType="nomin", InTerm=1, President="George W. Bush")
print("Hillary Clinton")
speech.df=tbl_df(sentence.list)%>%
filter(File=="HillaryClinton", type=="nomin", word.count>=4)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("Barack Obama")
speech.df=tbl_df(sentence.list)%>%
filter(File=="BarackObama", type=="nomin", Term==1, word.count>=5)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("George W Bush")
speech.df=tbl_df(sentence.list)%>%
filter(File=="GeorgeWBush", type=="nomin", Term==1, word.count>=4)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
print("Donald Trump")
speech.df=tbl_df(sentence.list)%>%
filter(File=="DonaldJTrump", type=="nomin", Term==1, word.count>=5)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
heatmap.2(cor(sentence.list%>%filter(type=="inaug")%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
presid.summary=tbl_df(sentence.list)%>%
filter(type=="nomin", File%in%sel.comparison)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
5)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
View(docs[[1]][[3]])
View(docs[[1]][[5]])
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:5,max.words=100
p
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:5,max.words=100)
?comparison.cloud
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:5,max.words=100,title.size=0.5)
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:5,max.words=100,title.size=1)
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:7,max.words=100,title.size=1)
?nrc
nrc
get_sentiments("nrc")
class(get_sentiments("nrc")$sentiment)
factor(get_sentiments("nrc")$sentiment)
for(i in 1:58){
docs[[i]][[3]]<-sent_detect(docs[[i]][[1]])
docs[[i]][[2]]$sentence<-length(docs[[i]][[3]])
docs[[i]][[4]]<-word_count(docs[[i]][[3]])
docs[[i]][[3]]<-data.frame(text=docs[[i]][[3]],stringsAsFactors = FALSE)
docs[[i]][[5]]<-unnest_tokens(docs[[i]][[3]],word,text)
# dtms.list[[i]]$freq<-data.frame(word=names(dtms.list[[i]]$freq),fq=dtms.list[[i]]$freq,stringsAsFactors = FALSE)
# rownames(dtms.list[[i]]$freq)<-NULL
}
docs[[1]][[5]] %>%
inner_join(get_sentiments("nrc")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=1:10,max.words=100,title.size=1)
